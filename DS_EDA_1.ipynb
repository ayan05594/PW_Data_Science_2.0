{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
        "# predicting the quality of wine.\n",
        "\n",
        "\"\"\"\n",
        "Key features of the wine quality dataset:\n",
        "- Fixed acidity: Measures the amount of acid in wine, directly affecting taste and preservation.\n",
        "- Volatile acidity: Affects the wine's aroma, with higher levels contributing to unpleasant smells.\n",
        "- Citric acid: Affects wine flavor and preservation.\n",
        "- Residual sugar: Affects sweetness, which can influence the wine's taste profile.\n",
        "- Chlorides: Measures salt content and can affect wine taste.\n",
        "- Free sulfur dioxide: Used as a preservative, it influences the wineâ€™s shelf life.\n",
        "- Total sulfur dioxide: Provides information on the overall level of sulfur in wine.\n",
        "- Density: Relates to the amount of sugar in the wine, which affects alcohol content and sweetness.\n",
        "- pH: Impacts the taste and stability of the wine.\n",
        "- Sulphates: Affect the wine's aroma and overall taste.\n",
        "- Alcohol: Affects the flavor and body of the wine.\n",
        "- Quality: The target variable, representing wine quality on a scale from 0 to 10.\n",
        "\n",
        "Each feature influences the overall taste and characteristics of the wine, and their combination will help predict wine quality.\n",
        "\"\"\"\n",
        "\n",
        "# Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
        "# Discuss the advantages and disadvantages of different imputation techniques.\n",
        "\n",
        "\"\"\"\n",
        "Handling missing data techniques:\n",
        "1. **Mean/Median Imputation**: Replace missing values with the mean or median of the respective feature.\n",
        "   - **Advantages**: Simple and quick, keeps dataset size intact.\n",
        "   - **Disadvantages**: Can distort data distribution, especially with large amounts of missing data.\n",
        "2. **KNN Imputation**: Use k-nearest neighbors to impute missing values based on the nearest samples.\n",
        "   - **Advantages**: More sophisticated, captures relationships between features.\n",
        "   - **Disadvantages**: Computationally expensive, may introduce bias if k is poorly chosen.\n",
        "3. **Regression Imputation**: Predict missing values using regression models on the other features.\n",
        "   - **Advantages**: Leverages existing data patterns for imputation.\n",
        "   - **Disadvantages**: Complex, may not work well if relationships between features are weak.\n",
        "\"\"\"\n",
        "\n",
        "# Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
        "# analyzing these factors using statistical techniques?\n",
        "\n",
        "\"\"\"\n",
        "Key factors affecting student performance:\n",
        "- Study hours and time management.\n",
        "- Attendance and participation in classes.\n",
        "- Social environment and family background.\n",
        "- Availability of learning resources.\n",
        "- Psychological factors such as stress or motivation.\n",
        "\n",
        "To analyze these factors:\n",
        "- **Descriptive Statistics**: To summarize data and identify trends.\n",
        "- **Correlation Analysis**: To check the relationships between factors and performance.\n",
        "- **Regression Analysis**: To model the relationship between student performance and other factors.\n",
        "\"\"\"\n",
        "\n",
        "# Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
        "# did you select and transform the variables for your model?\n",
        "\n",
        "\"\"\"\n",
        "Feature engineering process for student performance:\n",
        "1. **Data Cleaning**: Handling missing data, removing outliers, and addressing inconsistencies.\n",
        "2. **Feature Selection**: Choose the most relevant features based on domain knowledge or statistical tests.\n",
        "3. **Feature Transformation**:\n",
        "   - **Normalization/Scaling**: To ensure features have similar magnitudes (e.g., using Min-Max or Standardization).\n",
        "   - **Encoding**: Convert categorical variables (e.g., gender, education level) to numerical format (e.g., using Label Encoding).\n",
        "   - **Binning**: Group continuous features into categories, such as age ranges.\n",
        "\"\"\"\n",
        "\n",
        "# Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
        "# of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
        "# these features to improve normality?\n",
        "\n",
        "\"\"\"\n",
        "EDA steps:\n",
        "1. **Load the dataset** and inspect the first few rows and columns.\n",
        "2. **Visualizations**: Use histograms or box plots to check the distribution of each feature.\n",
        "3. **Statistical Tests**: Apply tests like Shapiro-Wilk or Anderson-Darling for normality testing.\n",
        "\n",
        "Transformation options:\n",
        "- For non-normal features, apply **log transformation** or **square root transformation** to reduce skewness.\n",
        "\"\"\"\n",
        "\n",
        "# Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
        "# features. What is the minimum number of principal components required to explain 90% of the variance in\n",
        "# the data?\n",
        "\n",
        "# Steps for PCA:\n",
        "# 1. **Standardize the dataset**: Ensure all features are on the same scale (e.g., using StandardScaler).\n",
        "# 2. **Apply PCA**: Use the PCA function in scikit-learn to reduce dimensionality.\n",
        "# 3. **Analyze explained variance**: Plot a cumulative explained variance graph to determine the number of components needed to explain 90% of the variance.\n",
        "\n",
        "# Code:\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "wine_data = pd.read_csv('winequality.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = wine_data.drop('quality', axis=1)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance vs. Number of Components')\n",
        "plt.show()\n",
        "\n",
        "# Determine the number of components that explain 90% of the variance\n",
        "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.90) + 1\n",
        "print(f\"Minimum number of components to explain 90% of the variance: {n_components}\")\n"
      ],
      "metadata": {
        "id": "0B_lphGWq4px"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}