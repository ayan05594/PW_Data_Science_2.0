{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7nV0-aAUNtaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
        "- Overfitting: Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise. The model performs well on the training data but poorly on unseen data (test data) due to its inability to generalize.\n",
        "  Consequences: High performance on training data but poor generalization to new data.\n",
        "  Mitigation: Use more training data, apply cross-validation, simplify the model (e.g., use fewer parameters), and use regularization techniques.\n",
        "\n",
        "- Underfitting: Underfitting occurs when a model is too simple to capture the underlying patterns in the data. The model performs poorly on both training and test data.\n",
        "  Consequences: Poor performance on both training and test data.\n",
        "  Mitigation: Use a more complex model, train for longer, or reduce the regularization.\n",
        "\n",
        "Q2: How can we reduce overfitting? Explain in brief.\n",
        "Overfitting can be reduced by:\n",
        "1. Using more data for training.\n",
        "2. Applying regularization (e.g., L1, L2 regularization).\n",
        "3. Pruning the model (e.g., reducing the depth of decision trees).\n",
        "4. Using cross-validation to evaluate the model.\n",
        "5. Early stopping during training to prevent learning from noise.\n",
        "\n",
        "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "Underfitting occurs when the model is too simple to capture the patterns in the data.\n",
        "Scenarios where underfitting can occur:\n",
        "1. Using a linear model for non-linear data.\n",
        "2. Using too few features in the model.\n",
        "3. Too high regularization that prevents the model from fitting the data.\n",
        "4. Not training the model for enough iterations or epochs.\n",
        "\n",
        "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
        "- Bias refers to the error introduced by approximating a real-world problem with a simplified model. High bias means the model is too simple and underfits the data.\n",
        "- Variance refers to the model's sensitivity to small changes in the training data. High variance means the model is too complex and overfits the data.\n",
        "- The bias-variance tradeoff is the balance between bias and variance. A model with high bias and low variance underfits, while a model with low bias and high variance overfits.\n",
        "- The goal is to find a balance where both bias and variance are minimized, leading to a model that generalizes well.\n",
        "\n",
        "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
        "- Use cross-validation to check how the model performs on different subsets of data.\n",
        "- Plot learning curves to track training and validation error. If training error decreases while validation error increases, the model is overfitting.\n",
        "- If both training and validation error are high, the model is underfitting.\n",
        "- Compare model performance on training and test data. A large difference in performance indicates overfitting.\n",
        "\n",
        "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
        "- High bias (underfitting) occurs when the model is too simple. Example: Linear regression for non-linear data.\n",
        "- High variance (overfitting) occurs when the model is too complex. Example: A very deep decision tree that learns noise in the training data.\n",
        "- High bias models have low performance on both training and test data, while high variance models perform well on training data but poorly on test data.\n",
        "\n",
        "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages overly complex models.\n",
        "- L1 Regularization (Lasso): Adds a penalty proportional to the absolute value of the coefficients, leading to sparse models (some coefficients may become zero).\n",
        "- L2 Regularization (Ridge): Adds a penalty proportional to the square of the coefficients, which discourages large coefficients.\n",
        "- Elastic Net: A combination of L1 and L2 regularization.\n",
        "Regularization helps to reduce model complexity and improve generalization by preventing the model from fitting noise in the data.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "-77AweH2gVzh",
        "outputId": "e4da3f8c-d82f-4f4d-b0a4-a217ac513148"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nQ1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\\n- Overfitting: Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise. The model performs well on the training data but poorly on unseen data (test data) due to its inability to generalize.\\n  Consequences: High performance on training data but poor generalization to new data.\\n  Mitigation: Use more training data, apply cross-validation, simplify the model (e.g., use fewer parameters), and use regularization techniques.\\n\\n- Underfitting: Underfitting occurs when a model is too simple to capture the underlying patterns in the data. The model performs poorly on both training and test data.\\n  Consequences: Poor performance on both training and test data.\\n  Mitigation: Use a more complex model, train for longer, or reduce the regularization.\\n\\nQ2: How can we reduce overfitting? Explain in brief.\\nOverfitting can be reduced by:\\n1. Using more data for training.\\n2. Applying regularization (e.g., L1, L2 regularization).\\n3. Pruning the model (e.g., reducing the depth of decision trees).\\n4. Using cross-validation to evaluate the model.\\n5. Early stopping during training to prevent learning from noise.\\n\\nQ3: Explain underfitting. List scenarios where underfitting can occur in ML.\\nUnderfitting occurs when the model is too simple to capture the patterns in the data.\\nScenarios where underfitting can occur:\\n1. Using a linear model for non-linear data.\\n2. Using too few features in the model.\\n3. Too high regularization that prevents the model from fitting the data.\\n4. Not training the model for enough iterations or epochs.\\n\\nQ4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\\n- Bias refers to the error introduced by approximating a real-world problem with a simplified model. High bias means the model is too simple and underfits the data.\\n- Variance refers to the model's sensitivity to small changes in the training data. High variance means the model is too complex and overfits the data.\\n- The bias-variance tradeoff is the balance between bias and variance. A model with high bias and low variance underfits, while a model with low bias and high variance overfits.\\n- The goal is to find a balance where both bias and variance are minimized, leading to a model that generalizes well.\\n\\nQ5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\\n- Use cross-validation to check how the model performs on different subsets of data.\\n- Plot learning curves to track training and validation error. If training error decreases while validation error increases, the model is overfitting.\\n- If both training and validation error are high, the model is underfitting.\\n- Compare model performance on training and test data. A large difference in performance indicates overfitting.\\n\\nQ6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\\n- High bias (underfitting) occurs when the model is too simple. Example: Linear regression for non-linear data.\\n- High variance (overfitting) occurs when the model is too complex. Example: A very deep decision tree that learns noise in the training data.\\n- High bias models have low performance on both training and test data, while high variance models perform well on training data but poorly on test data.\\n\\nQ7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\\nRegularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages overly complex models.\\n- L1 Regularization (Lasso): Adds a penalty proportional to the absolute value of the coefficients, leading to sparse models (some coefficients may become zero).\\n- L2 Regularization (Ridge): Adds a penalty proportional to the square of the coefficients, which discourages large coefficients.\\n- Elastic Net: A combination of L1 and L2 regularization.\\nRegularization helps to reduce model complexity and improve generalization by preventing the model from fitting noise in the data.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpO8gL65hH7G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}