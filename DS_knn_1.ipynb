{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xCbavmZTBEAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. What is the KNN algorithm?\n",
        "# The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric supervised learning method\n",
        "# used for classification and regression. It works by finding the K nearest data points to a given query point\n",
        "# based on a distance metric (e.g., Euclidean distance) and predicts the output based on the majority class\n",
        "# (classification) or the average value (regression) of these neighbors.\n",
        "\n",
        "# Q2. How do you choose the value of K in KNN?\n",
        "# Choosing the value of K involves a balance:\n",
        "# - A small K (e.g., K=1) makes the model sensitive to noise, potentially leading to overfitting.\n",
        "# - A large K smoothens the decision boundary but may cause underfitting.\n",
        "# Cross-validation is commonly used to select an optimal K value by testing multiple K values and\n",
        "# measuring the model's performance.\n",
        "\n",
        "# Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "# - **KNN Classifier**: Predicts the class label of a data point based on the majority class among its K nearest neighbors.\n",
        "# - **KNN Regressor**: Predicts a continuous value by averaging the values of the K nearest neighbors.\n",
        "\n",
        "# Q4. How do you measure the performance of KNN?\n",
        "# - **Classification**: Use metrics such as accuracy, precision, recall, F1 score, or the confusion matrix.\n",
        "# - **Regression**: Use metrics such as Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared (RÂ²).\n",
        "\n",
        "# Q5. What is the curse of dimensionality in KNN?\n",
        "# As the number of features (dimensions) increases, the data points become sparse, making it harder to identify\n",
        "# meaningful neighbors. The distance between points becomes less discriminative, which can degrade the performance of KNN.\n",
        "\n",
        "# Q6. How do you handle missing values in KNN?\n",
        "# - **Imputation**: Fill missing values using mean, median, or mode of the respective feature.\n",
        "# - **KNN Imputation**: Use KNN to estimate missing values by finding similar samples based on other features.\n",
        "\n",
        "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "# - **KNN Classifier**: Performs well in classification tasks where class labels are distinctly separable.\n",
        "# - **KNN Regressor**: Suitable for regression tasks where the relationship between features and the target is smooth and well-defined.\n",
        "# Both perform poorly in high-dimensional or sparse datasets unless dimensionality reduction techniques are applied.\n",
        "\n",
        "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks?\n",
        "# - **Strengths**:\n",
        "#   - Simple and easy to understand.\n",
        "#   - Non-parametric: No assumptions about data distribution.\n",
        "#   - Effective for small datasets.\n",
        "# - **Weaknesses**:\n",
        "#   - Sensitive to irrelevant features and noise.\n",
        "#   - Computationally expensive during prediction due to distance calculations.\n",
        "#   - Performance deteriorates in high-dimensional datasets (curse of dimensionality).\n",
        "# - **Solutions**:\n",
        "#   - Use feature selection or dimensionality reduction techniques.\n",
        "#   - Scale features using normalization or standardization.\n",
        "#   - Use approximate nearest neighbor techniques for large datasets.\n",
        "\n",
        "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "# - **Euclidean Distance**: Measures the straight-line distance between two points in space. Suitable for continuous data.\n",
        "# - **Manhattan Distance**: Measures the distance along axes at right angles (L1 distance). Suitable for categorical or grid-like data.\n",
        "\n",
        "# Q10. What is the role of feature scaling in KNN?\n",
        "# Feature scaling ensures that all features contribute equally to the distance metric. Without scaling, features with larger\n",
        "# ranges may dominate the distance calculation, leading to biased results. Techniques include:\n",
        "# - **Normalization**: Scaling values to a [0, 1] range.\n",
        "# - **Standardization**: Scaling values to have zero mean and unit variance.\n"
      ],
      "metadata": {
        "id": "pU9bvm1vuO8M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oA8XFmzKZ8Ll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}