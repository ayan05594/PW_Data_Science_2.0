{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xCbavmZTBEAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN?\n",
        "# How might this difference affect the performance of a KNN classifier or regressor?\n",
        "# - **Euclidean Distance**: Measures the straight-line (L2 norm) distance between two points in a space.\n",
        "#   - More sensitive to large feature magnitudes.\n",
        "#   - Better for continuous data where relationships are smooth and geometric.\n",
        "# - **Manhattan Distance**: Measures the distance along grid-like paths (L1 norm).\n",
        "#   - Less sensitive to large feature magnitudes.\n",
        "#   - Suitable for data where movements or changes occur in orthogonal directions.\n",
        "# **Effect on KNN**:\n",
        "# - For high-dimensional data or grid-like structures, Manhattan distance may perform better as it is less affected by feature scaling.\n",
        "# - Euclidean distance is more intuitive for continuous spaces with natural geometric relationships.\n",
        "\n",
        "# Q2. How do you choose the optimal value of k for a KNN classifier or regressor?\n",
        "# What techniques can be used to determine the optimal k value?\n",
        "# - Use cross-validation to test different values of k and evaluate performance metrics like accuracy (classification) or MSE (regression).\n",
        "# - Plot an \"elbow curve\" showing the relationship between k values and model performance.\n",
        "# - Choose a value of k that balances bias and variance (avoids overfitting or underfitting).\n",
        "\n",
        "# Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor?\n",
        "# In what situations might you choose one distance metric over the other?\n",
        "# - The distance metric determines how \"similarity\" is measured:\n",
        "#   - **Euclidean**: Best for continuous data and natural clusters.\n",
        "#   - **Manhattan**: Better for categorical or grid-aligned data.\n",
        "# - **Choice of Metric**:\n",
        "#   - If data has high-dimensionality or irregular scaling, Manhattan distance might be preferred.\n",
        "#   - For geometrically meaningful relationships, Euclidean distance works better.\n",
        "# - Custom distance metrics (e.g., Hamming distance) can be used for specific tasks like text or binary features.\n",
        "\n",
        "# Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model?\n",
        "# How might you go about tuning these hyperparameters to improve model performance?\n",
        "# - Common hyperparameters:\n",
        "#   - **k (number of neighbors)**: Controls the size of the neighborhood. Smaller k -> higher variance; Larger k -> higher bias.\n",
        "#   - **Distance Metric**: Affects how neighbors are calculated.\n",
        "#   - **Weighting Scheme**: Uniform (equal weights) or distance-based weights.\n",
        "# - **Tuning**:\n",
        "#   - Use grid search or random search to find the best combination of hyperparameters.\n",
        "#   - Perform cross-validation to evaluate and compare configurations.\n",
        "\n",
        "# Q5. How does the size of the training set affect the performance of a KNN classifier or regressor?\n",
        "# What techniques can be used to optimize the size of the training set?\n",
        "# - Larger training sets improve the model's generalization but increase computational cost.\n",
        "# - Smaller training sets reduce computation time but may not represent the data distribution well.\n",
        "# **Optimization**:\n",
        "# - Use a representative subset of data by stratified sampling.\n",
        "# - Apply dimensionality reduction techniques (e.g., PCA) to reduce computational load.\n",
        "# - Use approximate nearest neighbor methods for scalability.\n",
        "\n",
        "# Q6. What are some potential drawbacks of using KNN as a classifier or regressor?\n",
        "# How might you overcome these drawbacks to improve the performance of the model?\n",
        "# - **Drawbacks**:\n",
        "#   - Computationally expensive for large datasets due to distance calculations.\n",
        "#   - Sensitive to irrelevant features and feature scaling.\n",
        "#   - Poor performance in high-dimensional spaces (curse of dimensionality).\n",
        "#   - Memory-intensive as it requires storing the entire training set.\n",
        "# - **Solutions**:\n",
        "#   - Use feature scaling (normalization or standardization) to ensure equal contribution from all features.\n",
        "#   - Apply dimensionality reduction (e.g., PCA or feature selection).\n",
        "#   - Implement approximate nearest neighbor algorithms (e.g., KD-Tree or Ball-Tree).\n",
        "#   - Use weighted KNN to prioritize closer neighbors for more accurate predictions.\n"
      ],
      "metadata": {
        "id": "pU9bvm1vuO8M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oA8XFmzKZ8Ll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}