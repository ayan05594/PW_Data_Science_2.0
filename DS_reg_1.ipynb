{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q1. Simple Linear Regression vs. Multiple Linear Regression:\n",
        "- **Simple Linear Regression**: This is a type of regression where there is a relationship between a single independent variable (X) and a dependent variable (Y). It assumes that Y is linearly dependent on X. The equation is: Y = β0 + β1*X.\n",
        "  Example: Predicting a person's weight based on their height.\n",
        "- **Multiple Linear Regression**: This is an extension of simple linear regression that involves multiple independent variables. The equation is: Y = β0 + β1*X1 + β2*X2 + ... + βn*Xn.\n",
        "  Example: Predicting a person's weight based on their height, age, and diet.\n",
        "\n",
        "Q2. Assumptions of Linear Regression:\n",
        "- **Linearity**: The relationship between the independent and dependent variable should be linear.\n",
        "- **Independence**: Observations must be independent of each other.\n",
        "- **Homoscedasticity**: Constant variance of errors (residuals) across all levels of the independent variable(s).\n",
        "- **Normality**: Errors should be normally distributed.\n",
        "- **No Multicollinearity**: No independent variable should be highly correlated with another independent variable.\n",
        "To check these assumptions:\n",
        "- **Linearity**: Use scatter plots or residual plots.\n",
        "- **Independence**: Check for autocorrelation with the Durbin-Watson test.\n",
        "- **Homoscedasticity**: Plot residuals vs. predicted values and check for any pattern.\n",
        "- **Normality**: Use histograms or Q-Q plots for residuals.\n",
        "- **Multicollinearity**: Use Variance Inflation Factor (VIF).\n",
        "\n",
        "Q3. Interpreting Slope and Intercept in Linear Regression:\n",
        "- **Slope (β1)**: The rate of change in the dependent variable for a one-unit change in the independent variable. It shows the strength and direction of the relationship.\n",
        "- **Intercept (β0)**: The value of the dependent variable when the independent variable is zero.\n",
        "Example: If we have a regression model predicting house prices based on square footage (X), the slope tells us how much the price (Y) increases for each additional square foot, and the intercept tells us the base price when square footage is zero.\n",
        "\n",
        "Q4. Gradient Descent:\n",
        "- **Gradient Descent** is an optimization algorithm used to minimize the cost function in machine learning. It starts with an initial set of parameters (e.g., weights in linear regression) and iteratively adjusts them in the direction of the steepest decrease in the cost function.\n",
        "- It's used in machine learning to find the optimal values of parameters (e.g., coefficients in linear regression) that minimize prediction error.\n",
        "\n",
        "Q5. Multiple Linear Regression:\n",
        "- **Multiple Linear Regression** is an extension of simple linear regression that uses two or more independent variables to predict a dependent variable. The model equation is: Y = β0 + β1*X1 + β2*X2 + ... + βn*Xn.\n",
        "- Unlike simple linear regression, which uses only one independent variable, multiple linear regression can handle more complex relationships by considering multiple predictors.\n",
        "\n",
        "Q6. Multicollinearity in Multiple Linear Regression:\n",
        "- **Multicollinearity** occurs when two or more independent variables in a multiple linear regression model are highly correlated, leading to unreliable coefficient estimates.\n",
        "- It can be detected using:\n",
        "  - **Correlation matrix**: Checking pairwise correlation between variables.\n",
        "  - **Variance Inflation Factor (VIF)**: A VIF above 10 indicates high multicollinearity.\n",
        "- To address multicollinearity:\n",
        "  - Remove one of the correlated variables.\n",
        "  - Combine correlated variables into a single predictor.\n",
        "  - Apply regularization techniques like Ridge or Lasso regression.\n",
        "\n",
        "Q7. Polynomial Regression:\n",
        "- **Polynomial Regression** is a form of regression that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial. It allows for the modeling of non-linear relationships.\n",
        "- Unlike linear regression, which assumes a straight-line relationship, polynomial regression can model curves by using polynomial terms (e.g., X^2, X^3).\n",
        "\n",
        "Q8. Advantages and Disadvantages of Polynomial Regression:\n",
        "- **Advantages**:\n",
        "  - Can model non-linear relationships that linear regression cannot.\n",
        "  - Flexible and can fit complex data patterns.\n",
        "- **Disadvantages**:\n",
        "  - Can lead to overfitting, especially with higher-degree polynomials.\n",
        "  - More computationally expensive than linear regression.\n",
        "- **When to use**: Use polynomial regression when the data shows a non-linear relationship and a higher-degree polynomial fits the data better without overfitting.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "0B_lphGWq4px",
        "outputId": "c7af7811-e1ac-49a2-b67d-5e784aeaa19f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nQ1. Simple Linear Regression vs. Multiple Linear Regression:\\n- **Simple Linear Regression**: This is a type of regression where there is a relationship between a single independent variable (X) and a dependent variable (Y). It assumes that Y is linearly dependent on X. The equation is: Y = β0 + β1*X.\\n  Example: Predicting a person's weight based on their height.\\n- **Multiple Linear Regression**: This is an extension of simple linear regression that involves multiple independent variables. The equation is: Y = β0 + β1*X1 + β2*X2 + ... + βn*Xn.\\n  Example: Predicting a person's weight based on their height, age, and diet.\\n\\nQ2. Assumptions of Linear Regression:\\n- **Linearity**: The relationship between the independent and dependent variable should be linear.\\n- **Independence**: Observations must be independent of each other.\\n- **Homoscedasticity**: Constant variance of errors (residuals) across all levels of the independent variable(s).\\n- **Normality**: Errors should be normally distributed.\\n- **No Multicollinearity**: No independent variable should be highly correlated with another independent variable.\\nTo check these assumptions:\\n- **Linearity**: Use scatter plots or residual plots.\\n- **Independence**: Check for autocorrelation with the Durbin-Watson test.\\n- **Homoscedasticity**: Plot residuals vs. predicted values and check for any pattern.\\n- **Normality**: Use histograms or Q-Q plots for residuals.\\n- **Multicollinearity**: Use Variance Inflation Factor (VIF).\\n\\nQ3. Interpreting Slope and Intercept in Linear Regression:\\n- **Slope (β1)**: The rate of change in the dependent variable for a one-unit change in the independent variable. It shows the strength and direction of the relationship.\\n- **Intercept (β0)**: The value of the dependent variable when the independent variable is zero.\\nExample: If we have a regression model predicting house prices based on square footage (X), the slope tells us how much the price (Y) increases for each additional square foot, and the intercept tells us the base price when square footage is zero.\\n\\nQ4. Gradient Descent:\\n- **Gradient Descent** is an optimization algorithm used to minimize the cost function in machine learning. It starts with an initial set of parameters (e.g., weights in linear regression) and iteratively adjusts them in the direction of the steepest decrease in the cost function.\\n- It's used in machine learning to find the optimal values of parameters (e.g., coefficients in linear regression) that minimize prediction error.\\n\\nQ5. Multiple Linear Regression:\\n- **Multiple Linear Regression** is an extension of simple linear regression that uses two or more independent variables to predict a dependent variable. The model equation is: Y = β0 + β1*X1 + β2*X2 + ... + βn*Xn.\\n- Unlike simple linear regression, which uses only one independent variable, multiple linear regression can handle more complex relationships by considering multiple predictors.\\n\\nQ6. Multicollinearity in Multiple Linear Regression:\\n- **Multicollinearity** occurs when two or more independent variables in a multiple linear regression model are highly correlated, leading to unreliable coefficient estimates.\\n- It can be detected using:\\n  - **Correlation matrix**: Checking pairwise correlation between variables.\\n  - **Variance Inflation Factor (VIF)**: A VIF above 10 indicates high multicollinearity.\\n- To address multicollinearity:\\n  - Remove one of the correlated variables.\\n  - Combine correlated variables into a single predictor.\\n  - Apply regularization techniques like Ridge or Lasso regression.\\n\\nQ7. Polynomial Regression:\\n- **Polynomial Regression** is a form of regression that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial. It allows for the modeling of non-linear relationships.\\n- Unlike linear regression, which assumes a straight-line relationship, polynomial regression can model curves by using polynomial terms (e.g., X^2, X^3).\\n\\nQ8. Advantages and Disadvantages of Polynomial Regression:\\n- **Advantages**:\\n  - Can model non-linear relationships that linear regression cannot.\\n  - Flexible and can fit complex data patterns.\\n- **Disadvantages**:\\n  - Can lead to overfitting, especially with higher-degree polynomials.\\n  - More computationally expensive than linear regression.\\n- **When to use**: Use polynomial regression when the data shows a non-linear relationship and a higher-degree polynomial fits the data better without overfitting.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pU9bvm1vuO8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}