{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xCbavmZTBEAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. What is Gradient Boosting Regression?\n",
        "# Gradient Boosting Regression is a machine learning technique used for regression tasks.\n",
        "# It builds an ensemble of weak learners (e.g., decision trees) sequentially, where each tree corrects\n",
        "# the residual errors of the previous trees. The algorithm optimizes a loss function (e.g., mean squared error)\n",
        "# using gradient descent.\n",
        "\n",
        "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy.\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a simple dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # Features\n",
        "y = 3 * X.squeeze() + np.random.randn(100) * 2  # Target with noise\n",
        "\n",
        "# Split dataset into training and testing\n",
        "split_index = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Gradient Boosting Regressor from scratch\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        \"\"\"\n",
        "        Initialize the Gradient Boosting Regressor.\n",
        "\n",
        "        Parameters:\n",
        "        n_estimators: Number of weak learners (decision trees).\n",
        "        learning_rate: Step size for updating predictions.\n",
        "        max_depth: Maximum depth of each weak learner.\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the Gradient Boosting Regressor.\n",
        "        \"\"\"\n",
        "        residual = y\n",
        "        for _ in range(self.n_estimators):\n",
        "            model = DecisionStump(max_depth=self.max_depth)\n",
        "            model.fit(X, residual)\n",
        "            predictions = model.predict(X)\n",
        "\n",
        "            # Update residuals\n",
        "            residual -= self.learning_rate * predictions\n",
        "\n",
        "            # Store the trained model\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict target values for input data X.\n",
        "        \"\"\"\n",
        "        predictions = np.zeros(X.shape[0])\n",
        "        for model in self.models:\n",
        "            predictions += self.learning_rate * model.predict(X)\n",
        "        return predictions\n",
        "\n",
        "class DecisionStump:\n",
        "    def __init__(self, max_depth=1):\n",
        "        \"\"\"\n",
        "        Initialize a simple decision stump.\n",
        "        \"\"\"\n",
        "        self.threshold = None\n",
        "        self.feature_index = None\n",
        "        self.left_value = None\n",
        "        self.right_value = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the decision stump to the data.\n",
        "        \"\"\"\n",
        "        best_loss = float('inf')\n",
        "        for feature_index in range(X.shape[1]):\n",
        "            for threshold in np.unique(X[:, feature_index]):\n",
        "                left_mask = X[:, feature_index] <= threshold\n",
        "                right_mask = ~left_mask\n",
        "\n",
        "                left_value = y[left_mask].mean() if left_mask.any() else 0\n",
        "                right_value = y[right_mask].mean() if right_mask.any() else 0\n",
        "\n",
        "                predictions = np.where(left_mask, left_value, right_value)\n",
        "                loss = mean_squared_error(y, predictions)\n",
        "\n",
        "                if loss < best_loss:\n",
        "                    best_loss = loss\n",
        "                    self.threshold = threshold\n",
        "                    self.feature_index = feature_index\n",
        "                    self.left_value = left_value\n",
        "                    self.right_value = right_value\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict using the fitted decision stump.\n",
        "        \"\"\"\n",
        "        left_mask = X[:, self.feature_index] <= self.threshold\n",
        "        return np.where(left_mask, self.left_value, self.right_value)\n",
        "\n",
        "# Train the model\n",
        "gbr = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1)\n",
        "gbr.fit(X_train, y_train)\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R-Squared:\", r2_score(y_test, y_pred))\n",
        "\n",
        "# Q3. Experiment with different hyperparameters to optimize performance\n",
        "# Use Grid Search or Random Search to test combinations of hyperparameters:\n",
        "# - Learning rate\n",
        "# - Number of estimators\n",
        "# - Maximum depth\n",
        "# Evaluate each combination using cross-validation and metrics such as MSE or R-squared.\n",
        "\n",
        "# Q4. What is a weak learner in Gradient Boosting?\n",
        "# A weak learner is a simple model (e.g., a shallow decision tree) that performs slightly better than random guessing.\n",
        "# Gradient Boosting combines these weak learners to form a strong predictive model.\n",
        "\n",
        "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "# Gradient Boosting sequentially trains models to correct the errors made by previous models.\n",
        "# Each new learner minimizes the residual errors, optimizing the overall model's performance.\n",
        "\n",
        "# Q6. How does Gradient Boosting build an ensemble of weak learners?\n",
        "# 1. Initialize the model with a constant prediction (e.g., the mean of the target variable).\n",
        "# 2. Add weak learners iteratively:\n",
        "#    - Compute residuals (gradient of the loss function).\n",
        "#    - Train a weak learner on the residuals.\n",
        "#    - Update the ensemble with the new learner.\n",
        "# 3. Repeat until the desired number of iterations is reached.\n",
        "\n",
        "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting?\n",
        "# 1. Define a loss function L(y, f(x)) to measure model performance.\n",
        "# 2. Initialize the model with a constant prediction f_0(x).\n",
        "# 3. For each iteration t:\n",
        "#    - Compute the gradient of the loss function with respect to predictions g_t.\n",
        "#    - Train a weak learner h_t(x) to approximate the negative gradient -g_t.\n",
        "#    - Update the model: f_t(x) = f_{t-1}(x) + η * h_t(x), where η is the learning rate.\n",
        "# 4. Output the final ensemble model after T iterations.\n"
      ],
      "metadata": {
        "id": "pU9bvm1vuO8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oA8XFmzKZ8Ll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}