{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q1. R-squared in Linear Regression:\n",
        "- **R-squared (R²)** represents the proportion of variance in the dependent variable that is explained by the independent variable(s) in a regression model. It is calculated as:\n",
        "  R² = 1 - (SS_res / SS_tot)\n",
        "  Where SS_res is the sum of squared residuals (errors), and SS_tot is the total sum of squares (the variance in the dependent variable).\n",
        "- R² ranges from 0 to 1, with 1 indicating a perfect fit and 0 meaning no explanatory power. A higher R² value generally indicates a better fit.\n",
        "\n",
        "Q2. Adjusted R-squared:\n",
        "- **Adjusted R-squared** is a modified version of R-squared that adjusts for the number of predictors in the model. It is calculated as:\n",
        "  Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
        "  Where:\n",
        "  - n is the number of data points,\n",
        "  - p is the number of predictors.\n",
        "- Adjusted R-squared is useful for comparing models with different numbers of predictors because it penalizes adding irrelevant predictors that do not improve the model's performance.\n",
        "\n",
        "Q3. When to Use Adjusted R-squared:\n",
        "- **Adjusted R-squared** is more appropriate when comparing models with different numbers of independent variables (predictors). It helps to prevent the overestimation of model quality when more predictors are added, even if they do not provide additional explanatory power.\n",
        "\n",
        "Q4. RMSE, MSE, and MAE:\n",
        "- **Root Mean Squared Error (RMSE)**: Measures the square root of the average of squared residuals. It penalizes large errors more heavily than smaller ones. RMSE = √(1/n Σ (y_i - ŷ_i)²).\n",
        "- **Mean Squared Error (MSE)**: Measures the average of squared residuals. MSE = (1/n) Σ (y_i - ŷ_i)².\n",
        "- **Mean Absolute Error (MAE)**: Measures the average of absolute residuals. MAE = (1/n) Σ |y_i - ŷ_i|.\n",
        "- These metrics quantify the difference between the actual values (y_i) and the predicted values (ŷ_i), with RMSE and MSE being sensitive to larger errors and MAE being more robust to outliers.\n",
        "\n",
        "Q5. Advantages and Disadvantages of RMSE, MSE, and MAE:\n",
        "- **RMSE**:\n",
        "  - **Advantages**: Sensitive to large errors, which makes it useful when large errors are more undesirable.\n",
        "  - **Disadvantages**: Not robust to outliers; large errors disproportionately affect RMSE.\n",
        "- **MSE**:\n",
        "  - **Advantages**: Similar to RMSE, it penalizes larger errors more, and is easier to compute for mathematical optimization.\n",
        "  - **Disadvantages**: Not interpretable in the original units of the data.\n",
        "- **MAE**:\n",
        "  - **Advantages**: Less sensitive to outliers, provides a more interpretable result in the original units.\n",
        "  - **Disadvantages**: Does not penalize larger errors as much, which may not be desirable in some cases.\n",
        "\n",
        "Q6. Lasso vs. Ridge Regularization:\n",
        "- **Lasso (Least Absolute Shrinkage and Selection Operator)**: Lasso regularization adds a penalty term based on the absolute values of the coefficients (L1 norm). This results in some coefficients becoming exactly zero, effectively performing feature selection.\n",
        "- **Ridge Regularization**: Adds a penalty term based on the squared values of the coefficients (L2 norm), which shrinks the coefficients but does not set them to zero.\n",
        "- **When to Use**:\n",
        "  - **Lasso** is more suitable when you suspect that only a few predictors are important (sparse models).\n",
        "  - **Ridge** is more appropriate when you expect that all predictors have some influence but want to reduce their impact to prevent overfitting.\n",
        "\n",
        "Q7. Regularized Linear Models and Overfitting:\n",
        "- **Regularized linear models** (like Lasso and Ridge) help prevent overfitting by adding a penalty to the size of the coefficients. This discourages large coefficients that could result in overly complex models that fit noise rather than the underlying data pattern.\n",
        "- Example: In a linear regression model with many features, applying Ridge regularization will shrink the weights, preventing the model from fitting random fluctuations in the data.\n",
        "\n",
        "Q8. Limitations of Regularized Linear Models:\n",
        "- **Limitations**: Regularized models (like Lasso and Ridge) may not perform well if the underlying relationship is non-linear or if the number of features is very small. In cases where feature selection is unnecessary, Lasso can discard useful variables. Moreover, these models may not generalize well if the regularization parameter is not tuned properly.\n",
        "\n",
        "Q9. Choosing Between Models Using RMSE and MAE:\n",
        "- Model A (RMSE = 10) vs. Model B (MAE = 8):\n",
        "  - The choice of the better model depends on the problem context:\n",
        "    - **If large errors are undesirable**, Model B may be preferred since MAE is more robust to large errors.\n",
        "    - **If large errors are penalized more heavily**, Model A (with a lower RMSE) would be the better performer.\n",
        "  - There are trade-offs: RMSE penalizes large errors more heavily, but it may not reflect the overall error distribution as well as MAE does.\n",
        "\n",
        "Q10. Choosing Between Ridge and Lasso Regularization:\n",
        "- Model A (Ridge, λ = 0.1) vs. Model B (Lasso, λ = 0.5):\n",
        "  - **Ridge** tends to work better when all predictors have some influence, and the model is more stable when predictors are highly correlated.\n",
        "  - **Lasso** is suitable when you want to perform feature selection by setting some coefficients to zero.\n",
        "  - **Trade-offs**: Ridge is more suitable when you don't want to discard features, while Lasso is better for reducing the number of predictors. The regularization strength (λ) also plays a crucial role in controlling the degree of regularization.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "0B_lphGWq4px",
        "outputId": "97581e9d-9b12-46a4-f9e0-2c405ec88715"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nQ1. R-squared in Linear Regression:\\n- **R-squared (R²)** represents the proportion of variance in the dependent variable that is explained by the independent variable(s) in a regression model. It is calculated as:\\n  R² = 1 - (SS_res / SS_tot)\\n  Where SS_res is the sum of squared residuals (errors), and SS_tot is the total sum of squares (the variance in the dependent variable).\\n- R² ranges from 0 to 1, with 1 indicating a perfect fit and 0 meaning no explanatory power. A higher R² value generally indicates a better fit.\\n\\nQ2. Adjusted R-squared:\\n- **Adjusted R-squared** is a modified version of R-squared that adjusts for the number of predictors in the model. It is calculated as:\\n  Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\\n  Where:\\n  - n is the number of data points,\\n  - p is the number of predictors.\\n- Adjusted R-squared is useful for comparing models with different numbers of predictors because it penalizes adding irrelevant predictors that do not improve the model's performance.\\n\\nQ3. When to Use Adjusted R-squared:\\n- **Adjusted R-squared** is more appropriate when comparing models with different numbers of independent variables (predictors). It helps to prevent the overestimation of model quality when more predictors are added, even if they do not provide additional explanatory power.\\n\\nQ4. RMSE, MSE, and MAE:\\n- **Root Mean Squared Error (RMSE)**: Measures the square root of the average of squared residuals. It penalizes large errors more heavily than smaller ones. RMSE = √(1/n Σ (y_i - ŷ_i)²).\\n- **Mean Squared Error (MSE)**: Measures the average of squared residuals. MSE = (1/n) Σ (y_i - ŷ_i)².\\n- **Mean Absolute Error (MAE)**: Measures the average of absolute residuals. MAE = (1/n) Σ |y_i - ŷ_i|.\\n- These metrics quantify the difference between the actual values (y_i) and the predicted values (ŷ_i), with RMSE and MSE being sensitive to larger errors and MAE being more robust to outliers.\\n\\nQ5. Advantages and Disadvantages of RMSE, MSE, and MAE:\\n- **RMSE**:\\n  - **Advantages**: Sensitive to large errors, which makes it useful when large errors are more undesirable.\\n  - **Disadvantages**: Not robust to outliers; large errors disproportionately affect RMSE.\\n- **MSE**:\\n  - **Advantages**: Similar to RMSE, it penalizes larger errors more, and is easier to compute for mathematical optimization.\\n  - **Disadvantages**: Not interpretable in the original units of the data.\\n- **MAE**:\\n  - **Advantages**: Less sensitive to outliers, provides a more interpretable result in the original units.\\n  - **Disadvantages**: Does not penalize larger errors as much, which may not be desirable in some cases.\\n\\nQ6. Lasso vs. Ridge Regularization:\\n- **Lasso (Least Absolute Shrinkage and Selection Operator)**: Lasso regularization adds a penalty term based on the absolute values of the coefficients (L1 norm). This results in some coefficients becoming exactly zero, effectively performing feature selection.\\n- **Ridge Regularization**: Adds a penalty term based on the squared values of the coefficients (L2 norm), which shrinks the coefficients but does not set them to zero.\\n- **When to Use**:\\n  - **Lasso** is more suitable when you suspect that only a few predictors are important (sparse models).\\n  - **Ridge** is more appropriate when you expect that all predictors have some influence but want to reduce their impact to prevent overfitting.\\n\\nQ7. Regularized Linear Models and Overfitting:\\n- **Regularized linear models** (like Lasso and Ridge) help prevent overfitting by adding a penalty to the size of the coefficients. This discourages large coefficients that could result in overly complex models that fit noise rather than the underlying data pattern.\\n- Example: In a linear regression model with many features, applying Ridge regularization will shrink the weights, preventing the model from fitting random fluctuations in the data.\\n\\nQ8. Limitations of Regularized Linear Models:\\n- **Limitations**: Regularized models (like Lasso and Ridge) may not perform well if the underlying relationship is non-linear or if the number of features is very small. In cases where feature selection is unnecessary, Lasso can discard useful variables. Moreover, these models may not generalize well if the regularization parameter is not tuned properly.\\n\\nQ9. Choosing Between Models Using RMSE and MAE:\\n- Model A (RMSE = 10) vs. Model B (MAE = 8):\\n  - The choice of the better model depends on the problem context:\\n    - **If large errors are undesirable**, Model B may be preferred since MAE is more robust to large errors.\\n    - **If large errors are penalized more heavily**, Model A (with a lower RMSE) would be the better performer.\\n  - There are trade-offs: RMSE penalizes large errors more heavily, but it may not reflect the overall error distribution as well as MAE does.\\n\\nQ10. Choosing Between Ridge and Lasso Regularization:\\n- Model A (Ridge, λ = 0.1) vs. Model B (Lasso, λ = 0.5):\\n  - **Ridge** tends to work better when all predictors have some influence, and the model is more stable when predictors are highly correlated.\\n  - **Lasso** is suitable when you want to perform feature selection by setting some coefficients to zero.\\n  - **Trade-offs**: Ridge is more suitable when you don't want to discard features, while Lasso is better for reducing the number of predictors. The regularization strength (λ) also plays a crucial role in controlling the degree of regularization.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pU9bvm1vuO8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}