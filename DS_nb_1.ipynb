{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Q1. What is Bayes' theorem?\n",
        "'''\n",
        "Bayes' Theorem is a mathematical formula used to update the probability of a hypothesis based on new evidence. It describes how the probability of a class (or hypothesis) can be revised in light of new data. The theorem provides a way to update our beliefs about the likelihood of an event occurring based on prior knowledge and new evidence.\n",
        "'''\n",
        "\n",
        "# Q2. What is the formula for Bayes' theorem?\n",
        "'''\n",
        "Bayes' Theorem can be written as:\n",
        "\n",
        "P(H|E) = (P(E|H) * P(H)) / P(E)\n",
        "\n",
        "Where:\n",
        "- P(H|E) is the **posterior probability**: the probability of the hypothesis (H) given the evidence (E).\n",
        "- P(E|H) is the **likelihood**: the probability of observing the evidence (E) given that the hypothesis (H) is true.\n",
        "- P(H) is the **prior probability**: the initial probability of the hypothesis before observing the evidence.\n",
        "- P(E) is the **evidence**: the total probability of the evidence, considering all possible hypotheses.\n",
        "\n",
        "The formula shows how we can update our belief about a hypothesis (H) based on the new evidence (E).\n",
        "'''\n",
        "\n",
        "# Q3. How is Bayes' theorem used in practice?\n",
        "'''\n",
        "Bayes' Theorem is widely used in many practical applications, such as:\n",
        "1. **Spam email filtering**: Bayes' Theorem helps classify emails as spam or not based on prior knowledge about the characteristics of spam and non-spam emails.\n",
        "2. **Medical diagnosis**: It helps determine the likelihood of a disease (e.g., cancer) given the test results and prior probabilities (e.g., the base rate of the disease in the population).\n",
        "3. **Machine learning**: Naive Bayes classifiers, which apply Bayes' Theorem under the assumption that features are independent, are used for classification tasks like text classification, sentiment analysis, etc.\n",
        "4. **Predictive analytics**: It helps in updating predictions based on new evidence and is used in a variety of fields like finance, marketing, and artificial intelligence.\n",
        "\n",
        "In practice, Bayes' Theorem allows us to continuously update probabilities as new data is available, helping to make better-informed decisions.\n",
        "'''\n",
        "\n",
        "# Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
        "'''\n",
        "Bayes' Theorem is directly based on conditional probability. Conditional probability refers to the probability of an event occurring given that another event has already occurred.\n",
        "\n",
        "In Bayes' Theorem:\n",
        "- P(H|E) represents the **conditional probability** of the hypothesis H being true given the evidence E.\n",
        "- P(E|H) represents the **likelihood** of the evidence E given that the hypothesis H is true.\n",
        "\n",
        "Thus, Bayes' Theorem allows us to reverse conditional probabilities â€” to compute P(H|E) from P(E|H), given the prior probability of the hypothesis (P(H)) and the total probability of the evidence (P(E)).\n",
        "'''\n",
        "\n",
        "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
        "'''\n",
        "There are different types of Naive Bayes classifiers, and the choice of which one to use depends on the type of data and the assumptions about the distribution of the features:\n",
        "\n",
        "1. **Gaussian Naive Bayes**:\n",
        "   - Used when the features are continuous and assumed to follow a Gaussian (normal) distribution. It is the most common type used for continuous data.\n",
        "\n",
        "2. **Multinomial Naive Bayes**:\n",
        "   - Used when the features are discrete counts (e.g., word counts in text classification). It assumes that the feature values are generated from a multinomial distribution.\n",
        "\n",
        "3. **Bernoulli Naive Bayes**:\n",
        "   - Used when the features are binary (e.g., presence or absence of a feature). It assumes that the features follow a Bernoulli distribution (0 or 1 values).\n",
        "\n",
        "To choose the appropriate Naive Bayes classifier:\n",
        "- If your data is continuous and follows a normal distribution, use **Gaussian Naive Bayes**.\n",
        "- If your data consists of discrete counts (e.g., document-word frequencies), use **Multinomial Naive Bayes**.\n",
        "- If your features are binary (e.g., binary attributes), use **Bernoulli Naive Bayes**.\n",
        "'''\n",
        "\n",
        "# Q6. Assignment:\n",
        "# Given dataset with X1, X2 as features and two classes A and B, we want to classify a new instance (X1=3, X2=4).\n",
        "\n",
        "# Step-by-step solution:\n",
        "# Given data:\n",
        "# Class A: X1=1: 3, X1=2: 3, X1=3: 4, X2=1: 4, X2=2: 3, X2=3: 3, X2=4: 3\n",
        "# Class B: X1=1: 2, X1=2: 2, X1=3: 1, X2=1: 2, X2=2: 2, X2=3: 2, X2=4: 3\n",
        "\n",
        "# Given new instance: X1 = 3, X2 = 4\n",
        "\n",
        "# Step 1: Calculate the likelihood for each class.\n",
        "\n",
        "# Likelihood for Class A:\n",
        "P(X1=3 | A) = 4/16\n",
        "P(X2=4 | A) = 3/16\n",
        "\n",
        "# Likelihood for Class B:\n",
        "P(X1=3 | B) = 1/10\n",
        "P(X2=4 | B) = 3/10\n",
        "\n",
        "# Step 2: Compute the posterior probabilities for each class assuming equal priors P(A) = P(B) = 0.5\n",
        "\n",
        "# For Class A:\n",
        "P(A | X1=3, X2=4) = P(X1=3 | A) * P(X2=4 | A) * P(A)\n",
        "                   = (4/16) * (3/16) * 0.5\n",
        "                   = 0.0375\n",
        "\n",
        "# For Class B:\n",
        "P(B | X1=3, X2=4) = P(X1=3 | B) * P(X2=4 | B) * P(B)\n",
        "                   = (1/10) * (3/10) * 0.5\n",
        "                   = 0.015\n",
        "\n",
        "Step 3: Compare the posterior probabilities.\n",
        "- P(A | X1=3, X2=4) = 0.0375\n",
        "- P(B | X1=3, X2=4) = 0.015\n",
        "\n",
        "Since the posterior probability for Class A is higher, the Naive Bayes classifier would predict that the new instance belongs to **Class A**.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0B_lphGWq4px",
        "outputId": "02b40bd5-9768-446b-8b34-6d68494744db"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n**Example**: Cancer Diagnosis.\\n\\nIn this scenario, recall is crucial because:\\n- We want to minimize the number of actual cancer cases that are missed (false negatives), as early detection is vital for treatment.\\n- False negatives in this context can lead to delayed treatment or missed opportunities for curing the disease. Thus, high recall ensures that most patients who have cancer are identified.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pU9bvm1vuO8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}