{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is Random Forest Regressor?\n",
        "Random Forest Regressor is an ensemble learning method used for regression tasks. It builds multiple decision trees during training and combines their outputs (usually by averaging) to produce a more accurate and stable prediction. It is based on the bagging technique, where different subsets of the dataset are used to train each tree.\n",
        "\n",
        "# Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
        "Random Forest reduces overfitting by:\n",
        "1. Using bagging (Bootstrap Aggregation): Each tree is trained on a different random subset of the data.\n",
        "2. Introducing randomness in feature selection: Each split in a tree considers only a random subset of features, reducing the correlation between the trees.\n",
        "3. Aggregating predictions: The final output is an average of all trees' predictions, smoothing out errors from individual trees.\n",
        "\n",
        "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
        "Random Forest Regressor aggregates predictions by averaging the outputs of all decision trees in the forest. For example, if there are \\( n \\) trees and their predictions for a data point are \\( y_1, y_2, \\dots, y_n \\), the final prediction is:\n",
        "\\[\n",
        "\\text{Prediction} = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
        "\\]\n",
        "\n",
        "# Q4. What are the hyperparameters of Random Forest Regressor?\n",
        "Key hyperparameters include:\n",
        "1. **n_estimators**: The number of trees in the forest.\n",
        "2. **max_depth**: The maximum depth of each tree.\n",
        "3. **min_samples_split**: The minimum number of samples required to split an internal node.\n",
        "4. **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
        "5. **max_features**: The maximum number of features considered for splitting a node.\n",
        "6. **bootstrap**: Whether bootstrap sampling is used when building trees.\n",
        "7. **oob_score**: Whether to use out-of-bag samples to estimate generalization accuracy.\n",
        "\n",
        "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
        "1. **Overfitting**:\n",
        "   - Decision Tree Regressor is prone to overfitting, especially on noisy datasets.\n",
        "   - Random Forest Regressor reduces overfitting by averaging multiple trees.\n",
        "2. **Stability**:\n",
        "   - Decision Tree predictions can vary significantly with small changes in the data.\n",
        "   - Random Forest is more stable due to ensemble learning.\n",
        "3. **Performance**:\n",
        "   - Random Forest generally achieves better predictive performance than a single decision tree.\n",
        "4. **Complexity**:\n",
        "   - Decision Tree is simpler and faster to train.\n",
        "   - Random Forest requires more computational resources and training time.\n",
        "\n",
        "# Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
        "**Advantages**:\n",
        "1. Handles high-dimensional data and does not require feature scaling.\n",
        "2. Reduces overfitting and provides more accurate predictions.\n",
        "3. Can handle both numerical and categorical features.\n",
        "4. Provides feature importance metrics.\n",
        "\n",
        "**Disadvantages**:\n",
        "1. Computationally intensive, especially with a large number of trees.\n",
        "2. May not perform well on sparse datasets.\n",
        "3. Interpretability is lower compared to a single decision tree.\n",
        "\n",
        "# Q7. What is the output of Random Forest Regressor?\n",
        "The output of Random Forest Regressor is a continuous numeric value. It represents the average of predictions made by all the decision trees in the forest.\n",
        "\n",
        "# Q8. Can Random Forest Regressor be used for classification tasks?\n",
        "No, Random Forest Regressor is specifically designed for regression tasks where the output is a continuous numeric value. However, a closely related model, **Random Forest Classifier**, is used for classification tasks where the output is a categorical label. Both models are based on the same underlying principles, with the primary difference being the type of output and the aggregation method (e.g., majority voting for classification versus averaging for regression).\n"
      ],
      "metadata": {
        "id": "xCbavmZTBEAx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pU9bvm1vuO8M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}